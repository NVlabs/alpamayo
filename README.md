# Alpamayo-R1

<p align="left">
       ðŸ¤— <a href="https://huggingface.co/nvidia/Alpamayo-R1-10B">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspðŸ“‘ <a href="https://arxiv.org/pdf/2511.00088">Paper</a>&nbsp&nbsp
</p>

Inference code for Alpamayo-R1: a reasoning Vision-Language-Action (VLA) model for autonomous driving.

## Requirements

- Python 3.12.\*
- CUDA-capable GPU (requires at least 24GB of VRAM for bfloat16 inference)
- [uv](https://docs.astral.sh/uv/) package manager

## Environment Setup

### 1. Install uv (if not already installed)

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
export PATH="$HOME/.local/bin:$PATH"
```

### 2. Set up the environment

```bash
# Create virtual environment and install dependencies
uv venv ar1_venv
source ar1_venv/bin/activate
uv sync --active
```

### 3. Authenticate with HuggingFace

The model requires access to a gated dataset. First, request access at:
https://huggingface.co/datasets/nvidia/PhysicalAI-Autonomous-Vehicles

Then authenticate:

```bash
hf auth login
```

Get your token at: https://huggingface.co/settings/tokens

## Running Inference

### Test script

NOTE: This script will download both some example data (relatively small) and the model weights (22 GB).
The latter can be particularly slow depending on network bandwidth.
For reference, it takes around 2.5 minutes on a 100 MB/s wired connection.

```bash
python src/alpamayo_r1/test_inference.py
```

In case you would like to obtain more trajectories and reasoning traces, please feel free to change
the `num_traj_samples=1` argument to a higher number (Line 60).

### Interactive notebook

We provide a notebook with similar inference code at `notebook/inference.ipynb`.

## Project Structure

```
alpamayo_r1_release/
â”œâ”€â”€ notebook/
â”‚   â””â”€â”€ inference.ipynb                  # Example notebook
â”œâ”€â”€ src/
â”‚   â””â”€â”€ alpamayo_r1/
â”‚       â”œâ”€â”€ action_space/
â”‚       â”‚   â””â”€â”€ ...                      # Action space definitions
â”‚       â”œâ”€â”€ diffusion/
â”‚       â”‚   â””â”€â”€ ...                      # Diffusion model components
â”‚       â”œâ”€â”€ geometry/
â”‚       â”‚   â””â”€â”€ ...                      # Geometry utilities and modules
â”‚       â”œâ”€â”€ models/
â”‚       â”‚   â”œâ”€â”€ ...                      # Model components and utils functions
â”‚       â”œâ”€â”€ __init__.py                  # Package marker
â”‚       â”œâ”€â”€ config.py                    # Model and experiment configuration
â”‚       â”œâ”€â”€ helper.py                    # Utility functions
â”‚       â”œâ”€â”€ load_physical_aiavdataset.py # Dataset loader
â”‚       â”œâ”€â”€ test_inference.py            # Inference test script
â”œâ”€â”€ pyproject.toml                       # Project dependencies
â””â”€â”€ uv.lock                              # Locked dependency versions
```

## Troubleshooting

### Flash Attention issues

The model uses Flash Attention 2 by default. If you encounter compatibility issues:

```python
# Use PyTorch's scaled dot-product attention instead
config.attn_implementation = "sdpa"
```

## License

Apache License 2.0 - see [LICENSE](./LICENSE) for details.

## Disclaimer

Alpamayo-R1 is a pre-trained reasoning model designed to accelerate research and development in the autonomous vehicle (AV) domain. It is intended to serve as a foundation for a range of AV-related use cases-from instantiating an end-to-end backbone for autonomous driving to enabling reasoning-based auto-labeling tools. In short, it should be viewed as a building block for developing customized AV applications.

Important notes:

- Alpamayo-R1 is provided solely for research, experimentation, and evaluation purposes.
- Alpamayo-R1 is not a fully fledged driving stack. Among other limitations, it lacks access to critical real-world sensor inputs, does not incorporate required diverse and redundant safety mechanisms, and has not undergone automotive-grade validation for deployment.

By using this model, you acknowledge that it is a research tool intended to support scientific inquiry, benchmarking, and explorationâ€”not a substitute for a certified AV stack. The developers and contributors disclaim any responsibility or liability for the use of the model or its outputs.
